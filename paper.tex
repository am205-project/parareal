%File: paper.tex
%Created: Sat Dec 06 04:00 PM 2014 E
%
\documentclass[letterpaper,12pt]{article}
% Change the header if you're going to change this.
 
% Possible packages - uncomment to use
\usepackage{amsmath} % Needed for math stuff
%\usepackage{lastpage}sss% Finds last page
\usepackage{amssymb} % Needed for some math symbols
\usepackage{graphicx}% Needed for graphics
\usepackage[usenames,dvipsnames]{xcolor} % Needed for graphics and color
%\usepackage{setspace}sss% Needed to set single/double spacing
\usepackage{float} % Better placement of figures & tables
\usepackage{hyperref} % Can have actual links
\usepackage{mathpazo}
\usepackage[linesnumbered,vlined,ruled]{algorithm2e} % algorithm

%Suggested by TeXworks
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{verbatim} % lets you do verbatim text
 
% Sets the page margins to 1 inch each side
\usepackage[margin=1in]{geometry}
\geometry{letterpaper}
%\addtolength{\oddsidemargin}{-0.875in} 
%\addtolength{\evensidemargin}{-0.875in} 
%\addtolength{\textwidth}{1.75in} 
%\addtolength{\topmargin}{-.875in} 
%\addtolength{\textheight}{1.75in}

\frenchspacing

% Uncomment this section if you wish to have a header.
\usepackage{fancyhdr} 
\pagestyle{fancy} 
\renewcommand{\headrulewidth}{0.5pt} % customize the layout... 
\lhead{Chen, W., Sim, B., Shi, A.} \chead{Applied Math 205 Final Project} 
\rhead{Fall 2014} 
\lfoot{} \cfoot{\thepage} \rfoot{}


\begin{document}
\title{Explorations to Optimize the Parareal Algorithm \\to Solve ODEs in Given Applications}
\author{Wesley Chen, Brandon Sim, Andy Shi \\
Harvard University, Applied Math 205 Final Project}
\date{December 10, 2014}

% No indentation for new paragraphs
\setlength\parindent{0pt}

% Space between each new paragraph
\setlength\parskip{2ex}

% Put your stuff here
%\twocolumn[
%\begin{@twocolumnfalse}
\maketitle
\begin{abstract}

We look to achieve strong scaling in parallelizing numerical ODE methods. We test both parallelism by result space as well as parallelism by time, with the Parareal algorithm, to solve various ODEs.  We measure performance and compare in
terms of accuracy, speedup and efficiency. We provide some theoretical analysis for the Parareal algorithm and compare its tradeoff to paralleism by space, For the Parareal agorithm, we observe
light speedups in our small scale tests up to 64 processors on Harvard's
Odyssey cluster. We analyze possible reasons as to why our implementation
may not be ideal and some tradeoffs that are taken in the Parareal
implementation. Further optimizations are proposed and rationalized as next
steps including ports of to C++ using BLAS. Our code is written in python using mpi4py. We compare tests from various data sources from basic exponential ODEs to larger grids of ODEs to solve for in applications like diffusion and sound wave propagation.

\end{abstract}
%\end{@twocolumnfalse}
%]

\clearpage

%\tableofcontents

%\clearpage

\section{Background}

\subsection{Numerical Methods and Parallelization}
Solving systems of differential equations can be a computationally expensive
task. The error of most algorithms scales on the stepsize of the discretization
of time. However, stepsize in time is also proportional to the computation
required. It would be nice to allow for strong scaling where a problem can be
solved in a reasonable time for very small time steps.

Another obstacle to trying to parallelize numerical methods is that the methods
are inherently serial in time---in that evaluation of the next time point $n+1$
depends on the previous values, say those at time $n-1$ and $n$. This setup
does not allow for parallelism. The only way to incorporate parallelism is to
create schemes that have parallelizable components---such as an update or a
refinement built on top of a more basic method first computed in serial. As will be discussed and analyzed in the
metodology section, one such scheme would be the parareal algorithm. As an
overview, the parareal algorithm first does a coarse (low-order and fast)
approximation, then seeks to iteratively correct using smaller, more refined
numerical methods done in parallel from approximated starting points
approximated by the coarse solution.

An alternative paradigm to applying parallelism to numerical methods is the
parallelism by space paradigm, where the result vector space is divided up to
different regions and sent to different responsible processors. In this way,
each processor sees a similar but smaller-scale problem---weak scaling. This
method is expected to outperform the parallel in time paradigm simply because
the task is embarassingly parallel, being able to be naturally divided
into smaller but identical in method problems. There is no serial component in
this algorithm and should be exhibit both strong and weak scaling.

In general, the division in space paradigm is more intuitive to parallelize and
will often lead to greater speedups because there is no serial component. The main tradeoff however, is that knowing how to divide the resultant system requires knowledge of problem-specific details. Thus, this approach lacks the
general stability and theoretical proofs (of error and convergence) that schemes which parallelize in
time can offer.

\subsection{Measuring Parallel Performance and Amdahl's Law}

Performance of parallel algorithms are usually measured in terms of 3 different metrics: speedup, strong scalling efficiency and weak scaling efficiency. 

\section{Methodology}

The parareal algorithm, developed by Lions, Maday and Turinici in 2001, CITE is
a generalized algorithm which allows for parallelization in time. It does so
by using a cheaper ($g_{\Delta t}$)---a lower order or lesser resolution
approximation first, which it then corrects by evaluating segments with a finer
method $g_{\textnormal{fine}}$. The correction step from the approximation can
be done in parallel. Multiple iterations can be performed to achieve an error
provably asymptotically equal to a full serial computation of
$g_{\textnormal{fine}}$. The exact number of iterations required to achieve
certain levels of convergence depends on the problem and must be tuned top
optimize for speedup for a given system. In summary, the algorithm is $k$
repetitions of a correction process which requires running updated by a finer
method run in parallel for subsections.

\begin{algorithm}[t]
    \KwIn{Temporal discretization $t_n = t_0 + n \Delta t, \, n = 1,2,\ldots,N$}
    \KwIn{Coarse scheme $g_{\Delta t}$}
    \KwIn{Finer scheme $g_{\textnormal{fine}}$}
    Compute $u^1_{n+1} = g_{\Delta t}(t_n, u^1_n)$\;
    Compute the corrections $\delta g(u^1_n) = g_{\textnormal{fine}}(t_n,
    u^1_n) - g_{\Delta t}(t_n, u^1_n)$ in parallel\;
    Add the prediction and correction terms as $u^2_{n+1} = g_{\Delta t}(t_n,
    u^2_n) + \delta g(u^1_n)$\;
    Repeat steps 2 and 3, incrementing the iteration label and using $u^{k+1}_0
    = u^1_0$ as the initial condition\;
 \caption{Parareal}
 \label{alg:parareal}
\end{algorithm}

The beauty of the parareal method is that it can be adapted into many different versions with different choices of the $g_{\Delta t}$ and $g_{\textnormal{fine}}$ solution operators. The $g_{\textnormal{fine}}$ solution is a more computationally expensive method but assumed to have lesser error.  Most of the time this will be a higher order method, but can also be a greater sampling of the same order method used in the coarse version.  The exact optimum pairing is problem-specific and the code is written so that a different "n-order method step" function could be written and then replaced into the code. We experiment with testing different pairs of $g_{\Delta t}$ and $g_{\textnormal{fine}}$ to see how influential the choice of solution scheme is.

\subsection{Visualization of Parareal Scheme}

bksim TODO put a graph because maybe it'll help? the slides have a version

\subsection{Convergence of the Parareal}

Assume the coarse operator $g_{\Delta t}$ is Lipschitz and order $m$: for some
constant $L$, 
\[ |g_{\Delta t}(t_n, u) - g_{\Delta t}(t_n, v)| \leq (1 + L \Delta t) |u - v|
\, \forall t \in (0, t_n), \]
\[ |u(t_n) - u^1_N | \leq C(\Delta t)^m |u_0|. \]

Additionally, assume the function $u$ is bounded on $(0, t_n)$. Furthermore,
assume the fine solution operator $g_{\textnormal{fine}}$ is a sufficiently
accurate approximation to the analytic operator so we may replace
$g_{\textnormal{fine}} \to g$. 

\textbf{Theorem}: The order of accuracy of the Parareal method with coarse
solution operator $g_{\Delta t}$ and fine operator $g$ is $mk$. We can prove
this using induction [Bal]. 

\emph{Proof}: By induction. 

Case $k = 1$: This is just the coarse operator, which is order $m$. 

Assume that this is true for $k$, that $|u(t_N) - u^k_N| \leq C(\Delta t)^{mk}
|u_0|$. 

For the case $k + 1$, we have
\[
\begin{aligned}
    |u(t_N) - u^{k+1}_N| &= 
    |g(u(t_{N-1})) - g_{\Delta t}(u^{k+1}_{N-1}) - \delta g(u^k_{N-1})| \\
    &= |g_{\Delta t}(u(t_{N-1})) - g_{\Delta t}(u^{k+1}_{N-1}) - \delta
    g(u^k_{N-1}) + \delta g(u(t_{N-1}))| \\
    &\leq |g_{\Delta t}(u(t_{N-1})) - g_{\Delta t}(u^{k+1}_{N-1})| - |\delta
    g(u^k_{N-1}) + \delta g(u(t_{N-1}))| \\
    &\leq (1 + C \Delta t) |u(t_{N-1} - u_{N-1}^{k+1}| + C(\Delta t)^{m+1} |
    u_{N-1}^k - u(t_{N-1}) | \\
    &\leq (1 + C \Delta t) |u(t_{N-1}) - u_{N-1}^{k+1}| + C(\Delta t)^{m(k+1)+1}
    | u_0 | \\
\end{aligned}
\]

We can continue to expand $|u(t_{N-1} - u_{N-1}^{k+1}|$ and get time indices of
$N-1, N-2, \ldots, 2, 1$. This implies that 
\[ |u(t_N) - u^{k+1}_N| \leq C(\Delta t)^{m(k+1)} |u_0|, \]
as desired. 

\subsection{Error of the Parareal}

With the assumptions of the previous section, this parareal method will
approach, with large enough $k$ (correction iterations) to approach the error of
the fine method. However, there is a time vs. accuracy tradeoff. Let $Q$ be the
\emph{Quality Factor} for $g_{\Delta t}$ and $g_{\textnormal{fine}}$. Suppose
that, for constant number of processors and $\Delta t$, $g_{\Delta t}$ runs in
time $T$. Then, $g_{\textnormal{fine}}$ runs in time $QT$. With too large of a
$k$ and a lower $Q$, the time for the Parareal could potentially take longer
than the direct serial computation. We note that there could be other
relationships between the time of the coarse and fine schemes other than
multiplication by a constant term. 

Disregarding the time relationship between $g_{\Delta t}$ and
$g_{\textnormal{fine}}$, we can look at what happens to the error as $k \to N$
(recall that $N$ is the total number of time points in the time discretization). 

For $k = 1, 2, \ldots$ we have 
\[u_{n+1}^{k+1} = g_{\Delta t} (t_n, u_n^{k+1}) + \big(
g_{\textnormal{fine}}(t_n, u_n^k) - g_{\Delta t}(t_n, u_n^k). \]

As $k \to N$, the Parareal algorithm gives $u_{n}^{k+1} = u^k_n$, so the order
of accuracy approaches that of $g_{\textnormal{fine}}$. 

% I don't understand the above part ~ Andy 

\subsection{Stability of the Parareal}

With the Parareal method, it is possible to combine ODE solvers. The stability region depends on both $g_{\Delta t}$ and $g_{\textnormal{fine}}$, and the
equation being solved. 

FROM THE SLIDES 

ASHI carry?


\subsection{Parallel Tradeoff Analysis}

We look to compute a theoretical maximum speedup enabled by the nature of the Parareal algorithm.  As this is a tradeoff between error (with parameter k being the number of iterations of corrections, each which takes another cycle of $g_{\Delta t}$, we seek to express the tradeoff as a function of the quality factor, $Q$, the number of iterations, $k$ and the number of processors $n$.  Our quality factor is defined as the multiplicative factor of additional time necessary to 

Then, the runtime of Parareal is, assuming negligible setup and
aggregation time,

TODO WESLEY

\begin{equation}
t + k \left(t + \frac{Qt}{N} \right).
\end{equation}

The first t seconds comes from the first coarse approximation, without which the parareal algorithm degenerates to $g_{\Delta t}$,  In order for there to be a speedup relative to the fine operator, we require
that $t + k(t + Qt/N) < Qt$, or $k < \frac{Q - 1}{1 + Q/N}$ or $N > \frac{Qk}{Q
- 1 - k}$. This can be tuned with by either adjusting the the quality factor or reducing the number of iterations, but the number of iterations, k, is important for error convergence to that of $g_{\textnormal{fine}}$.  However, this optimal k is a problem-specific parameter and must be optimized for.

TODO PARALLEL IN SPACE ANALYSIS

\subsection{Incorporating Communication}

TODO WESLEY

TODO PARALLEL IN SPACE ANALYSIS

\subsection{Amdahl's Law Analysis}

TODO WESLEY

TODO PARALLEL IN SPACE ANALYSIS

\subsection{Explored Tests}

We wished to explore certain tests that would span the space of possible
parameters to the parareal as well as possible applications in different ODE
systems. Because there were so many different versions and parameters for the Parareal to test,

\section{Results and Discussion}

\subsection{Expectations}
TODO WESLEY

\subsection{General Remarks about Results}
LETS SEE IF I HAVE TO DISCLAIM ANY GLOBAL FAILURES IN OUR RESULTS

\subsection{Comparison to Serial with $g_{\textnormal{fine}}$ as a Forward Euler with
Smaller $\Delta t$}

We implemented the Parareal algorithm in Python, using mpi4py to parallelize it.
Our coarse operator was forward Euler step, with 100 steps, while the fine
operator was forward Euler, with $Q * 100$ steps, where $Q$ is the quality
factor. We tested the Parareal algorithm on two sets of differential equations:

\[
y'(t) = f(t, y) = \lambda y, \, y(0) = 1
\]
\[
y''(t) + 2y'(t) + 5y(t) = 0, \, y(0) = 1 \, y'(0) = 0.
\]

We ran the algorithm on different numbers of processors and varied $k$ and $Q$. 

Varying Iterations:
]
Varying Quality Factor:
We define the quality factor in this case for how many times smaller the Euler method used for $g_{\textnormal{fine}}$ is compared to the Euler method used for $g_{\Delta t}$

Changing the ODE

TODO ashi sort out the figure and match and replace if needbe

\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth]{data/runtime_vs_cpus.pdf}
\caption{Running time vs. number of CPUs.}
\label{fig:run_v_cpu}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.75\linewidth]{data/error_vs_corrections.pdf}
\caption{Error vs. number of correction steps.}
\label{fig:err_v_k}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.75\linewidth]{data/error_vs_qualityfactor.pdf}
\caption{Error vs. quality factor of the fine operator}
\label{fig:err_v_q}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.75\linewidth]{data/runtime_vs_corrections.pdf}
\caption{Running time as a function of the number of correction steps.}
\label{fig:run_v_k}
\end{center}
\end{figure}

\subsection{Comparison to Serial with $g_{\textnormal{fine}}$ as Higher Order Methods}

TODO team

IDEALLY WE SHOW THE SAME GRAPHS AS THE ABOVE WITH ALL THE VARIATIONS TESTED

\subsection{Parallelism by Space Paradigm}

For the speaker problem, we designed a problem-specific method to parallelize in space and divide Pierce Hall into subsections such that each processor would receive a division of the building to compute. We wish to strong scaling, in which the computation time of our parallel method will decrease as we increase the number of processors. We compare this version to the serial version we did for Homework 4.

bksim TODO GET DATA, AS REQUIRED FROM ABOVE, SHOW GRAPHS

\subsection{Parallelism Paradigm Comparisons}

Now with 3 different parallel implementations, two in time with the parareal, one in space, we compare across the methods to see if we can make any statements about general performance of our different paradigms.

TODO WESLEY

TODO compare the computation times (and perhaps a slight change in accuracy of the results) between both parareal methods the best in the sections before hand

TODO bsim and ashi show graphs and some times for the forward Euler parareal
versions---the fastest ones or both if we have both implemented

\subsection{Comparison to C++ Implementations}

The numpy environment combined with the required structure of MPI required
certain type/data structure conversions which will take time.not not cunot cu

\subsection{Summary of Performance}

TODO team

\subsection{Demonstrating Scaling}

ashi TODO by taking some numbers and following

As we can see, we do see the effects of strong scaling

TODO analyze

\subsection{Possible Optimizations and Future Work}

We see that (HOPE IT WORKS WELL OR ELSE...) Most of the further work would be in
optimization. We are trying to measure performance in Python, which is not the
ideal benchmarking language for efficiency and speedups. A port over to C++
using the MPI libraries in C++ rather than the mpi4py libraries in python would
be ideal. Other optimizations mentioned above could also be implemented. To
further explore the parareal algorithm would involve many test cases and to see
how the parareal algorithm compares to other approaches---since the actual time
and iterations necessary depend on the system of ODEs to solve.

On the other hand, parallelism by space already requires a very problem-specific
design. The strong scaling efficiency of parallelism by space is high and is
conceptually embarrassingly parallel and easier to create. However, the
limitations are in the requirement for the code to be designed for the
problem---how to divide by space so as to allow for the maximum number of even
divisions. The lack of generality is not as beautiful as the parareal algorithm
but may lend itself to faster speedups.

TODO Wesley

\section{Conclusion and Future Work}


All in all, we have implemented and begun to explore some techniques of looking
for strong and weak scaling efficiencies for solving ODEs. The parareal
algorithm is beautiful in its theoretical advantages---in terms of stability,
error and efficiency. However, the method, as a very generalized method,
requires deeper analysis for the specific problem. The main variable will be in
the difference in the coarse and fine methods (be it lower and higher order or a
higher and lower resolution for the same method). The tradeoffs that must be
computed and optimized for would be problem specific.

\section{References}

Bibtex Citations for the Slides, and for \url{https://www.sharcnet.ca/help/index.php/Measuring_Parallel_Scaling_Performance}

%\bibliographystyle{unsrt}
%\bibliography{poster/sample}

Guilliame Bal paper

Stability of the Parareal Algorithm by Staff et al


\end{document}
